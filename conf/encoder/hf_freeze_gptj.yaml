# @package _group_

# model type. One of [hf_bert, pytext_bert, fairseq_roberta]
encoder_model_type: hf_freeze

# HuggingFace's config name for model initialization
pretrained_model_cfg: EleutherAI/gpt-neo-1.3B #bert-base-uncased

# Some encoders need to be initialized from a file
pretrained_file: "facebook/dpr-ctx_encoder-single-nq-base" #"/data/project/rw/DPR/downloads/checkpoint/retriever/single-adv-hn/nq/bert-base-encoder.cp" 

# Extra linear layer on top of standard bert/roberta encoder
projection_dim: 0

# Max length of the encoder input sequence
sequence_length: 256 

dropout: 0.1

# whether to fix (don't update) context encoder during training or not
fix_ctx_encoder: True 

# if False, the model won't load pre-trained BERT weights
pretrained: True
